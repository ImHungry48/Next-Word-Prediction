{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ImHungry48/Next-Word-Prediction/blob/main/Next_Word_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQuDU-2OgmRp"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from random import choices\n",
        "from enum import Enum\n",
        "\n",
        "class SentenceMutator():\n",
        "  class TYPO_TYPE(Enum):\n",
        "      SWAP = 1\n",
        "      SUBSTITUTE = 2\n",
        "      DELETE = 3\n",
        "      INSERT = 4\n",
        "      CASE = 5\n",
        "  \n",
        "  def __init__(self, texts, typo_chance=0.005):\n",
        "    # The chance of an individual character in a word being mutated\n",
        "    self.typo_chance = typo_chance\n",
        "\n",
        "    # All neighboring keys on a standard QWERTY keyboard\n",
        "    self.neighboring_keys = {\n",
        "        'q': 'was',\n",
        "        'w': 'qeasd',\n",
        "        'e': 'wrsdf',\n",
        "        'r': 'etdfg',\n",
        "        't': 'ryfgh',\n",
        "        'y': 'tughj',\n",
        "        'u': 'yihjk',\n",
        "        'i': 'uojkl',\n",
        "        'o': 'ipkl',\n",
        "        'p': 'ol',\n",
        "        'a': 'qwszx',\n",
        "        's': 'qwesz',\n",
        "        'd': 'wersfc',\n",
        "        'f': 'ertdgcv',\n",
        "        'g': 'rtyfhvb',\n",
        "        'h': 'tyugjbn',\n",
        "        'j': 'yuihknm',\n",
        "        'k': 'uiojlm',\n",
        "        'l': 'iopk',\n",
        "        'z': 'asx',\n",
        "        'x': 'sdzc',\n",
        "        'c': 'dfxv',\n",
        "        'v': 'fgcb',\n",
        "        'b': 'vghn',\n",
        "        'n': 'bhjm',\n",
        "        'm': 'njk',\n",
        "        '.': 'l;/\\',',\n",
        "        '?': ';\\'.',\n",
        "        '!': '12@wq`',\n",
        "        '-': '=[p0]'\n",
        "      }\n",
        "\n",
        "    self.word_as_list = []\n",
        "  \n",
        "  def mutate_sentence(self, sentence):\n",
        "    \"\"\" Mutates an entire sentence by swapping, substituting, deleting, inserting, \n",
        "        or changing the case of characters in the sentence's words.\n",
        "\n",
        "    Args:\n",
        "        sentence(str): The sentence to mutate\n",
        "    Returns:\n",
        "        mutated_sentence(str): The sentence after ungoing mutations. Value does not change\n",
        "        if no mutations occur\n",
        "    \"\"\"\n",
        "    \n",
        "    # Split the sentence into individual words\n",
        "    words = sentence.split()\n",
        "\n",
        "    # Create an array to store transformed words\n",
        "    typoed_words = []\n",
        "\n",
        "    # Mutate and store each word\n",
        "    for word in words:\n",
        "      typoed_words.append(self.mutate(word))\n",
        "    \n",
        "    # Return the sentence\n",
        "    return ' '.join(typoed_words)\n",
        "\n",
        "  def get_neighboring_keys(self, char, is_upper):\n",
        "    # Get all neighboring keys of the given character\n",
        "    neighbors = self.neighboring_keys[char]\n",
        "\n",
        "    # Return a random neighboring key\n",
        "    return random.choice(neighbors).upper() if is_upper else random.choice(neighbors)\n",
        "  \n",
        "  def swap(self, char_index, word):\n",
        "    # Determine index of character is being swapped\n",
        "    index_to_swap = char_index + 1\n",
        "\n",
        "    if char_index == len(word) - 1: # If the last letter\n",
        "      index_to_swap = len(word) - 2\n",
        "\n",
        "    # Perform the swap\n",
        "    temp = self.word_as_list[char_index]\n",
        "    self.word_as_list[char_index] = self.word_as_list[index_to_swap]\n",
        "    self.word_as_list[index_to_swap] = temp\n",
        "\n",
        "  def substitute(self, char, char_index):\n",
        "    # 80% chance to substitute with a neighboring character\n",
        "    sub_with_nearby = random.random() < 0.8\n",
        "\n",
        "    if sub_with_nearby:\n",
        "      # Get neighboring keys\n",
        "      sub_char = self.get_neighboring_keys(char.lower(), char.isupper())\n",
        "      \n",
        "    else:\n",
        "      # Get a random letter in the alphabet\n",
        "      characters = 'abcdefghijklmnopqrstuvwxyz!-=;,.\\'\\\\?'\n",
        "      sub_char = random.choice(list(characters))\n",
        "\n",
        "    # Perform the substitution\n",
        "    self.word_as_list[char_index] = sub_char\n",
        "\n",
        "  def delete(self, char_index): \n",
        "    # Perform a soft deletion\n",
        "    self.word_as_list[char_index] = -1\n",
        "\n",
        "  def insert(self, char_index, char):\n",
        "    # 80% chance to insert with a neighboring character\n",
        "    insert_with_nearby = random.random() < 0.8\n",
        "\n",
        "    if insert_with_nearby:\n",
        "      # Get neighboring keys\n",
        "      insert_char = self.get_neighboring_keys(char.lower(), char.isupper())\n",
        "    else:\n",
        "      # Get a random letter in the alphabet\n",
        "      characters = 'abcdefghijklmnopqrstuvwxyz!-=;,.\\'\\\\?'\n",
        "      insert_char = random.choice(list(characters))\n",
        "\n",
        "    # Perform the insertion\n",
        "    self.word_as_list.insert(char_index, insert_char)\n",
        "\n",
        "  def case(self, char, char_index):\n",
        "    # Change letter to opposite case\n",
        "    if char.isupper():\n",
        "      self.word_as_list[char_index] = char.lower()\n",
        "    else:\n",
        "      self.word_as_list[char_index] = char.upper()\n",
        "\n",
        "  def mutate(self, word):\n",
        "    \"\"\" Mutates a single word by swapping, substituting, deleting, inserting, \n",
        "        or changing the case of characters.\n",
        "\n",
        "    Args:\n",
        "        word(str): The word to mutate.\n",
        "    Returns:\n",
        "        mutated_word(str): The word after ungoing mutations. Value does not change\n",
        "        if no mutations occur\n",
        "    \"\"\"\n",
        "    \n",
        "    # Make the word a list for easier manipulation\n",
        "    word_as_list = list(word)\n",
        "\n",
        "    for char_index in range(len(word)):    \n",
        "      char = word[char_index]    \n",
        "      if random.random() <= self.typo_chance:\n",
        "        # Select a random typo type\n",
        "        typo_type= random.choices(list(self.TYPO_TYPE),\n",
        "                              weights = [3, 4, 3, 4, 1])[0]\n",
        "        \n",
        "        # Implement the randomly selected typo type\n",
        "        if typo_type == self.TYPO_TYPE.SWAP:\n",
        "\n",
        "          # Only swap if there are enough characters in the word\n",
        "          if len(word) < 2:\n",
        "            break\n",
        "\n",
        "          # Perform the swap\n",
        "          self.swap(char_index)\n",
        "\n",
        "        elif typo_type == self.TYPO_TYPE.SUBSTITUTE:\n",
        "          # Perform the substitution\n",
        "          self.substitute(char_index)\n",
        "\n",
        "        elif typo_type == self.TYPO_TYPE.DELETE:\n",
        "          # Only delete if there are enough characters in the word\n",
        "          if len(word) < 2:\n",
        "            break\n",
        "\n",
        "          # Perform the deletion\n",
        "          self.delete(char_index)\n",
        "\n",
        "        elif typo_type == self.TYPO_TYPE.INSERT:\n",
        "          # Perform the insertion\n",
        "          self.insert(char_index, char)\n",
        "\n",
        "        elif typo_type == self.TYPO_TYPE.CASE.value:\n",
        "          # Perform the case change\n",
        "          self.case(char, char_index)\n",
        "\n",
        "    mutated_word = ''.join(char for char in word_as_list if type(char) == str)\n",
        "    return mutated_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SentenceDataset(Dataset):\n",
        "  def __init__(self, texts, tokenizer, max_length, augmentation=False, typo_chance=0.005):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.tokenized_sentences = []\n",
        "    self.augmentation = augmentation\n",
        "    self.mutator = SentenceMutator(texts, typo_chance)\n",
        "\n",
        "    # Initalize the sentences in the dataset\n",
        "    self.init_sentences(texts, typo_chance)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.tokenized_sentences)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    sentence = self.tokenized_sentences[idx]\n",
        "    result = torch.tensor(sentence)\n",
        "    return result\n",
        "\n",
        "  def init_sentences(self, texts, typo_chance):\n",
        "    for text in texts:\n",
        "      for sentence in sent_tokenize(text):\n",
        "        if self.augmentation:\n",
        "          sentence = self.mutator(texts, typo_chance)\n",
        "        sentence_tokens = tokenizer.encode(sentence, max_length=max_length, truncation=True)\n",
        "        self.tokenized_sentences.append(sentence_tokens)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self): \n",
        "        # TODO: Implement me!\n",
        "\n",
        "    def forward(self):\n",
        "        # TODO: Implement me!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.nn import TransformerEncoderLayer, TransformerEncoder\n",
        "import math\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, max_length, ntokens, d_model, nhead, nhid, nlayers):\n",
        "        self.max_length = max_length\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=nhid)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(max_length, d_model)\n",
        "        self.decoder = nn.Linear(d_model, ntokens)\n",
        "    \n",
        "    def forward(self, src):\n",
        "        # Process the src through the embedding and positional encoder\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "\n",
        "        # Apply the transformer encoder\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "\n",
        "        # Final linear layer to get predictions\n",
        "        output = self.decoder(output)\n",
        "\n",
        "        # Return the output\n",
        "        return output\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Initialize the GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Set model hyperparameters\n",
        "max_length = 256 # Set maximum sequence length for later padding\n",
        "ntokens = tokenizer.vocab_size # Size of vocabulary (num of unique tokens)\n",
        "d_model = 256 # Embedding dimension\n",
        "nhead = 4 # Number of attention heads\n",
        "nhid = 512 # Number of feedforward network hidden units\n",
        "nlayers = 4 # Number of Transformer layers\n",
        "#dropout = 0.01 # Dropout rate\n",
        "\n",
        "# Initialize the model\n",
        "model = TransformerModel(\n",
        "    ntokens=ntokens, \n",
        "    max_length=max_length, \n",
        "    d_model=d_model, \n",
        "    nhead=nhead, \n",
        "    nhid=nhid, \n",
        "    nlayers=nlayers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "num_epochs = 5\n",
        "batch_size = 1\n",
        "learning_rate = 5e-5\n",
        "\n",
        "# Set the path directory\n",
        "file_path = '/content/drive/MyDrive/untitled.txt'\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    dataset = file.readlines()\n",
        "    \n",
        "sentence_dataset = SentenceDataset(dataset, tokenizer, max_length)\n",
        "dataloader = DataLoader(sentence_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), learning_rate=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    for batch_idx, data in enumerate(dataloader):\n",
        "        inputs = data.to(torch.long)  # Assuming your data is of type Long\n",
        "        targets = inputs.clone()  # Adjust this line based on your task\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs.view(-1, ntokens), targets.view(-1))\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "    \n",
        "    average_loss = total_train_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOY7oYB14tx2tovEProg19q",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
